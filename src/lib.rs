#![no_std]

use svisual::{SV, NAME_SZ};
pub mod prelude;

use byteorder::{LE,ByteOrder};
use stm32f103xx_hal as hal;
use crate::hal::{
    device::{USART1,USART2,USART3},
    serial::Tx,
    dma::DmaChannel
};

fn copy_slice(dst: &mut [u8], src: &[u8]) {
    for (d, s) in dst.iter_mut().zip(src.iter()) {
        *d = *s;
    }
}


pub trait SendPackageDma<N, P> : DmaChannel
where
    N: heapless::ArrayLength<(&'static [u8], svisual::ValueRec<P>)>,
    P: generic_array::ArrayLength<i32> + typenum::marker_traits::Unsigned + core::ops::Mul<U4>,
    <P as core::ops::Mul<U4>>::Output : generic_array::ArrayLength<u8>
{
    fn send_package_dma(
        &mut self,
        module: &'static [u8],
        c: &mut Self::Dma,
        values: &SV<N, P>);
}

type GA<P> = GenericArray<u8, Prod<P, U4>>;
use generic_array::typenum::{Prod, consts::U4};
use generic_array::GenericArray;

macro_rules! impl_send_package_dma {
    ($USARTX:ident) => {
impl<N, P> SendPackageDma<N, P> for Tx<$USARTX>
where
    N: heapless::ArrayLength<(&'static [u8], svisual::ValueRec<P>)>,
    P: generic_array::ArrayLength<i32> + typenum::marker_traits::Unsigned + core::ops::Mul<U4>,
    <P as core::ops::Mul<U4>>::Output : generic_array::ArrayLength<u8>
{
    fn send_package_dma(
        &mut self,
        module: &'static [u8],
        c: &mut Self::Dma,
        values: &SV<N, P>)
    {
        //if values.map.is_empty() {
        //    return Err(Error::EmptyPackage);
        //}

        self.write_all_and_wait(c, b"=begin=");
        
        let packet_size = P::to_usize();
        let vl_size : usize = NAME_SZ+4+packet_size*4;
        static mut NDATA : [u8; NAME_SZ+4] = [0u8; NAME_SZ+4];
        let mut val_data : GA<P> = GenericArray::default();// should be static
        unsafe {
            // Full package size
            LE::write_i32(&mut NDATA[0..4], (NAME_SZ + vl_size * values.map.len()) as i32);
            // Identifier (name) of the module
            copy_slice(&mut NDATA[4..], module);
        }
        unsafe { self.write_all_and_wait(c, &NDATA) };

        for (k, v) in values.map.iter() {
            // Data for single variable
            unsafe {
                NDATA = [0u8; NAME_SZ+4];
                copy_slice(&mut NDATA[0..NAME_SZ], k);
                LE::write_i32(&mut NDATA[NAME_SZ..], v.vtype as i32);
            }
            unsafe { self.write_all_and_wait(c, &NDATA) };
            
            LE::write_i32_into(&v.vals, val_data.as_mut_slice());
            
            self.write_all_and_wait(c, &val_data);
        }
        self.write_all_and_wait(c, b"=end=");
    }
}
    }
}

impl_send_package_dma!(USART1);
impl_send_package_dma!(USART2);
impl_send_package_dma!(USART3);

use stable_deref_trait::StableDeref;
use as_slice::AsSlice;

use core::sync::atomic::{self, Ordering};
use cast::u16;
extern crate cast;


pub trait WriteDmaWait<B>: hal::dma::DmaChannel
where B: StableDeref + AsSlice<Element = u8>/* + 'static*/ {
    fn write_all_and_wait(&mut self, chan: &mut Self::Dma, buffer: B) -> B;
}

macro_rules! write_dma_wait {
    ($(
        $USARTX:ident: (
            $tcifX:ident,
            $cgifX:ident
        ),
    )+) => {
        $(
            impl<B> WriteDmaWait<B> for Tx<$USARTX> where B: StableDeref + AsSlice<Element = u8>/* + 'static*/ {
                fn write_all_and_wait(&mut self, chan: &mut Self::Dma, buffer: B
                ) -> B
                {
                    chan.mar().write(|w|
                        w.ma().bits(buffer.as_slice().as_ptr() as usize as u32)
                    );
                    chan.ndtr().write(|w|
                        w.ndt().bits(u16(buffer.as_slice().len()).unwrap())
                    );
                    chan.par().write(|w| unsafe {
                        w.pa().bits(&(*$USARTX::ptr()).dr as *const _ as usize as u32)
                    });

                    // TODO can we weaken this compiler barrier?
                    // NOTE(compiler_fence) operations on `buffer` should not be reordered after
                    // the next statement, which starts the DMA transfer
                    atomic::compiler_fence(Ordering::SeqCst);

                    chan.cr().modify(|_, w| { w
                        .mem2mem() .clear_bit()
                        .pl()      .medium()
                        .msize()   .bit8()
                        .psize()   .bit8()
                        .minc()    .set_bit()
                        .pinc()    .clear_bit()
                        .circ()    .clear_bit()
                        .dir()     .set_bit()
                        .en()      .set_bit()
                    });

                    // XXX should we check for transfer errors here?
                    // The manual says "A DMA transfer error can be generated by reading
                    // from or writing to a reserved address space". I think it's impossible
                    // to get to that state with our type safe API and *safe* Rust.
                    while !chan.isr().$tcifX().bit_is_set() {}

                    chan.ifcr().write(|w| w.$cgifX().set_bit());

                    chan.cr().modify(|_, w| w.en().clear_bit());

                    // TODO can we weaken this compiler barrier?
                    // NOTE(compiler_fence) operations on `buffer` should not be reordered
                    // before the previous statement, which marks the DMA transfer as done
                    atomic::compiler_fence(Ordering::SeqCst);

                    buffer
                }
            }
        )+
    }
}

write_dma_wait! {
    USART1: (
        tcif4,
        cgif4
    ),
    USART2: (
        tcif7,
        cgif7
    ),
    USART3: (
        tcif2,
        cgif2
    ),
}

